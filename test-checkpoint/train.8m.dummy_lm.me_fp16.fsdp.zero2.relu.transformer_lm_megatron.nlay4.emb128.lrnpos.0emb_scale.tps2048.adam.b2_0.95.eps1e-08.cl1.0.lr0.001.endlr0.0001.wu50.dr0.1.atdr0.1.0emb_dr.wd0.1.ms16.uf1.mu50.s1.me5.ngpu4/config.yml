_name: null
common:
  _name: null
  log_interval: 5
  log_format: json
  log_file: null
  tensorboard_logdir: ./test-checkpoint/train.8m.dummy_lm.me_fp16.fsdp.zero2.relu.transformer_lm_megatron.nlay4.emb128.lrnpos.0emb_scale.tps2048.adam.b2_0.95.eps1e-08.cl1.0.lr0.001.endlr0.0001.wu50.dr0.1.atdr0.1.0emb_dr.wd0.1.ms16.uf1.mu50.s1.me5.ngpu4
  aim_repo: null
  aim_run_hash: null
  wandb_project: null
  azureml_logging: false
  seed: 1
  cpu: false
  fp16: true
  memory_efficient_fp16: true
  bf16: false
  fp16_no_flatten_grads: false
  fp16_init_scale: 4
  fp16_scale_window: 256
  fp16_scale_tolerance: 0.0
  min_loss_scale: 0.03125
  threshold_loss_scale: null
  user_dir: null
  empty_cache_freq: 0
  all_gather_list_size: 16384
  model_parallel_size: 2
  profile: false
  use_plasma_view: false
  plasma_path: /tmp/plasma
  log_nvidia_smi: false
common_eval:
  _name: null
  path: null
  quiet: false
  model_overrides: '{}'
  results_path: null
distributed_training:
  _name: null
  distributed_world_size: 4
  distributed_rank: 0
  distributed_backend: nccl
  distributed_init_method: tcp://localhost:12201
  distributed_port: -1
  device_id: 0
  distributed_no_spawn: false
  ddp_backend: fully_sharded
  bucket_cap_mb: 25
  fix_batches_to_gpus: false
  find_unused_parameters: false
  fast_stat_sync: false
  broadcast_buffers: false
  zero_sharding: none
  fp16: true
  memory_efficient_fp16: true
  bf16: false
  no_reshard_after_forward: true
  fp32_reduce_scatter: false
  cpu_offload: false
  use_sharded_state: true
  gradient_predivide_factor: null
  distributed_num_procs: 4
dataset:
  _name: null
  num_workers: 1
  num_workers_valid: 1
  skip_invalid_size_inputs_valid_test: false
  max_tokens: null
  batch_size: 16
  required_batch_size_multiple: 1
  dataset_impl: null
  data_buffer_size: 10
  skip_batches: ''
  train_subset: train
  valid_subset: valid
  combine_valid_subsets: null
  ignore_unused_valid_subsets: true
  validate_interval_updates: 2000
  validate_after_updates: 0
  fixed_validation_seed: null
  disable_validation: true
  max_tokens_valid: null
  batch_size_valid: 16
  max_valid_steps: null
  gen_subset: test
  num_shards: 1
  shard_id: 0
optimization:
  _name: null
  max_epoch: 5
  max_update: 20
  clip_norm: 1.0
  clip_norm_type: l2
  skip_gradient_update_on_clip_norm: false
  ewm_ratio_to_skip_batch: -1.0
  update_freq:
  - 1
  lr:
  - 0.001
checkpoint:
  _name: null
  save_dir: ./test-checkpoint/train.8m.dummy_lm.me_fp16.fsdp.zero2.relu.transformer_lm_megatron.nlay4.emb128.lrnpos.0emb_scale.tps2048.adam.b2_0.95.eps1e-08.cl1.0.lr0.001.endlr0.0001.wu50.dr0.1.atdr0.1.0emb_dr.wd0.1.ms16.uf1.mu50.s1.me5.ngpu4
  restore_file: checkpoint_last.pt
  finetune_from_model: null
  reset_dataloader: false
  reset_lr_scheduler: false
  reset_meters: false
  reset_optimizer: false
  optimizer_overrides: '{}'
  save_interval_epochs: 0
  save_interval_updates: 0
  save_last_checkpoint: true
  keep_last_epochs: -1
  keep_last_updates: -1
  checkpoint_suffix: -model_part-0
  checkpoint_shard_count: 1
  load_checkpoint_on_all_dp_ranks: false
  write_checkpoints_asynchronously: true
  cloud_upload_path: null
  nfs_eval_script_path: null
  nfs_eval_num_attempts: 10
  nfs_eval_attempt_wait_minutes: 5
  nfs_eval_frequency: 5000
  cluster_env: azure
  model_parallel_size: 2
  sequence_parallel: false
generation:
  _name: null
  beam: 5
  max_len_a: 0.0
  max_len_b: 200
  min_len: 1
  sampling: false
  sampling_topp: -1.0
  temperature: 1.0
  no_seed_provided: false
  buffer_size: 0
  input: '-'
eval_lm:
  _name: null
  score_sequences: false
  output_word_probs: false
  output_word_stats: false
  context_window: 0
  softmax_batch: 9223372036854775807
  max_valid_steps: null
reshard:
  _name: null
  save_dir: ./test-checkpoint/train.8m.dummy_lm.me_fp16.fsdp.zero2.relu.transformer_lm_megatron.nlay4.emb128.lrnpos.0emb_scale.tps2048.adam.b2_0.95.eps1e-08.cl1.0.lr0.001.endlr0.0001.wu50.dr0.1.atdr0.1.0emb_dr.wd0.1.ms16.uf1.mu50.s1.me5.ngpu4
  save_prefix: reshard
  target_world_size: 128
  do_pad: false
task:
  _name: dummy_lm
  dict_size: 51196
  dataset_size: 100000
  tokens_per_sample: 2048
  add_bos_token: false
  batch_size: 16
  batch_size_valid: 16
  max_tokens: null
  max_target_positions: 2048
optimizer:
  _name: adam
  adam_betas: (0.9, 0.95)
  adam_eps: 1.0e-08
  weight_decay: 0.1
  use_old_adam: false
  fp16_adam_stats: false
  lr:
  - 0.001
lr_scheduler:
  _name: polynomial_decay
  warmup_updates: 50
  force_anneal: null
  end_learning_rate: 0.0001
  zero_lr_warmup_steps: 0
  power: 1.0
  total_num_update: 50.0
  lr:
  - 0.001
bpe: null
tokenizer: null
args:
  log_interval: 5
  log_format: json
  log_file: null
  tensorboard_logdir: ./test-checkpoint/train.8m.dummy_lm.me_fp16.fsdp.zero2.relu.transformer_lm_megatron.nlay4.emb128.lrnpos.0emb_scale.tps2048.adam.b2_0.95.eps1e-08.cl1.0.lr0.001.endlr0.0001.wu50.dr0.1.atdr0.1.0emb_dr.wd0.1.ms16.uf1.mu50.s1.me5.ngpu4
  aim_repo: null
  aim_run_hash: null
  wandb_project: null
  azureml_logging: false
  seed: 1
  cpu: false
  fp16: true
  memory_efficient_fp16: true
  bf16: false
  fp16_no_flatten_grads: false
  fp16_init_scale: 4
  fp16_scale_window: 256
  fp16_scale_tolerance: 0.0
  min_loss_scale: 0.03125
  threshold_loss_scale: null
  user_dir: null
  empty_cache_freq: 0
  all_gather_list_size: 16384
  model_parallel_size: 2
  profile: false
  use_plasma_view: false
  plasma_path: /tmp/plasma
  log_nvidia_smi: false
  criterion: vocab_parallel_cross_entropy
  optimizer: adam
  lr_scheduler: polynomial_decay
  tokenizer: null
  bpe: null
  task: dummy_lm
  num_workers: 1
  num_workers_valid: 1
  skip_invalid_size_inputs_valid_test: false
  max_tokens: null
  batch_size: 16
  required_batch_size_multiple: 1
  dataset_impl: null
  data_buffer_size: 10
  skip_batches: ''
  train_subset: train
  valid_subset: valid
  combine_valid_subsets: null
  ignore_unused_valid_subsets: true
  validate_interval_updates: 2000
  validate_after_updates: 0
  fixed_validation_seed: null
  disable_validation: true
  max_tokens_valid: null
  batch_size_valid: 16
  max_valid_steps: null
  gen_subset: test
  num_shards: 1
  shard_id: 0
  distributed_world_size: 4
  distributed_rank: 0
  distributed_backend: nccl
  distributed_init_method: null
  distributed_port: -1
  device_id: 0
  distributed_no_spawn: false
  ddp_backend: fully_sharded
  bucket_cap_mb: 25
  fix_batches_to_gpus: false
  find_unused_parameters: false
  fast_stat_sync: false
  broadcast_buffers: false
  zero_sharding: none
  no_reshard_after_forward: true
  fp32_reduce_scatter: false
  cpu_offload: false
  use_sharded_state: true
  gradient_predivide_factor: null
  arch: transformer_lm_megatron
  max_epoch: 5
  max_update: 20
  clip_norm: 1.0
  clip_norm_type: l2
  skip_gradient_update_on_clip_norm: false
  ewm_ratio_to_skip_batch: -1
  update_freq:
  - 1
  lr:
  - 0.001
  save_dir: ./test-checkpoint/train.8m.dummy_lm.me_fp16.fsdp.zero2.relu.transformer_lm_megatron.nlay4.emb128.lrnpos.0emb_scale.tps2048.adam.b2_0.95.eps1e-08.cl1.0.lr0.001.endlr0.0001.wu50.dr0.1.atdr0.1.0emb_dr.wd0.1.ms16.uf1.mu50.s1.me5.ngpu4
  restore_file: checkpoint_last.pt
  finetune_from_model: null
  reset_dataloader: false
  reset_lr_scheduler: false
  reset_meters: false
  reset_optimizer: false
  optimizer_overrides: '{}'
  save_interval_epochs: 0
  save_interval_updates: 0
  save_last_checkpoint: true
  keep_last_epochs: -1
  keep_last_updates: -1
  checkpoint_suffix: ''
  checkpoint_shard_count: 1
  load_checkpoint_on_all_dp_ranks: false
  write_checkpoints_asynchronously: true
  cloud_upload_path: null
  nfs_eval_script_path: null
  nfs_eval_num_attempts: 10
  nfs_eval_attempt_wait_minutes: 5
  nfs_eval_frequency: 5000
  cluster_env: azure
  sequence_parallel: false
  dict_size: 51196
  dataset_size: 100000
  tokens_per_sample: 2048
  add_bos_token: false
  adam_betas: (0.9, 0.95)
  adam_eps: 1.0e-08
  weight_decay: 0.1
  use_old_adam: false
  fp16_adam_stats: false
  warmup_updates: 50
  force_anneal: null
  end_learning_rate: 0.0001
  zero_lr_warmup_steps: 0
  power: 1.0
  total_num_update: '50'
  checkpoint_activations: true
  distribute_checkpointed_activations: true
  tensor_parallel_init_model_on_gpu: true
  full_megatron_init: true
  megatron_init_sigma: 0.006
  activation_fn: relu
  share_decoder_input_output_embed: true
  decoder_layers: 4
  decoder_embed_dim: 128
  decoder_ffn_embed_dim: 512
  decoder_attention_heads: 2
  decoder_learned_pos: true
  no_scale_embedding: true
  dropout: 0.1
  attention_dropout: 0.1
  no_emb_dropout: true
  no_seed_provided: false
  decoder_learned_sinusoidal: false
  _name: vocab_parallel_cross_entropy
